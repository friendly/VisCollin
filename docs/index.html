<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Visualizing Collinearity Diagnostics ‚Ä¢ VisCollin</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="‚Äùimage/svg+xml‚Äù" href="favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" sizes="any" href="favicon.ico">
<link rel="manifest" href="site.webmanifest">
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/Roboto-0.4.10/font.css" rel="stylesheet">
<link href="deps/JetBrains_Mono-0.4.10/font.css" rel="stylesheet">
<link href="deps/Roboto_Slab-0.4.10/font.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Visualizing Collinearity Diagnostics">
<meta name="description" content='Provides methods to calculate diagnostics for multicollinearity among predictors in a linear or generalized linear model. It also provides methods to visualize those diagnostics following Friendly &amp; Kwan (2009), "Where‚Äôs Waldo: Visualizing Collinearity Diagnostics", &lt;doi:10.1198/tast.2009.0012&gt;. These include better tabular presentation of collinearity diagnostics that highlight the important numbers, a semi-graphic tableplot of the diagnostics to make warning and danger levels more salient, and a "collinearity biplot" of the smallest dimensions of predictor space, where collinearity is most apparent.'>
<meta property="og:description" content='Provides methods to calculate diagnostics for multicollinearity among predictors in a linear or generalized linear model. It also provides methods to visualize those diagnostics following Friendly &amp; Kwan (2009), "Where‚Äôs Waldo: Visualizing Collinearity Diagnostics", &lt;doi:10.1198/tast.2009.0012&gt;. These include better tabular presentation of collinearity diagnostics that highlight the important numbers, a semi-graphic tableplot of the diagnostics to make warning and danger levels more salient, and a "collinearity biplot" of the smallest dimensions of predictor space, where collinearity is most apparent.'>
<meta property="og:image" content="https://friendly.github.io/VisCollin/logo.png">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">VisCollin</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.4</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/friendly/VisCollin/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9">
<div class="section level1">
<div class="page-header">
<img src="logo.png" class="logo" alt=""><h1 id="viscollin-">VisCollin <a class="anchor" aria-label="anchor" href="#viscollin-"></a>
</h1>
</div>
<p><strong>Visualizing Collinearity Diagnostics</strong></p>
<p>Version 0.1.2</p>
<p>The <code>VisCollin</code> package provides methods to calculate diagnostics for multicollinearity among predictors in a linear or generalized linear model. It also provides methods to visualize those diagnostics following Friendly &amp; Kwan (2009), ‚ÄúWhere‚Äôs Waldo: Visualizing Collinearity Diagnostics‚Äù, <em>The American Statistician</em>, <strong>63</strong>, 56‚Äì65.</p>
<p>These include:</p>
<ul>
<li>better <strong>tabular presentation</strong> of collinearity diagnostics that highlight the important numbers.</li>
<li>a semi-graphic <strong>tableplot</strong> of the diagnostics to make warning and danger levels more salient and</li>
<li>a <strong>collinearity biplot</strong> of the <em>smallest dimensions</em> of predictor space, where collinearity is most apparent.</li>
</ul>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<table class="table">
<colgroup>
<col width="30%">
<col width="70%">
</colgroup>
<tbody>
<tr>
<td>CRAN version</td>
<td><code>install.packages("VisCollin")</code></td>
</tr>
<tr>
<td>Development version</td>
<td><code>remotes::install_github("friendly/VisCollin")</code></td>
</tr>
</tbody>
</table>
</div>
<div class="section level2">
<h2 id="tutorial-example">Tutorial example<a class="anchor" aria-label="anchor" href="#tutorial-example"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/friendly/VisCollin" class="external-link">VisCollin</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyr.tidyverse.org" class="external-link">tidyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/" class="external-link">car</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/taiyun/corrplot" class="external-link">corrplot</a></span><span class="op">)</span></span></code></pre></div>
<p>This example uses the <code>cars</code> data set containing various measures of size and performance on 406 models of automobiles from 1982. Interest is focused on predicting gas mileage, <code>mpg</code>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">cars</span>, package <span class="op">=</span> <span class="st">"VisCollin"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">cars</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'data.frame':    406 obs. of  10 variables:</span></span>
<span><span class="co">#&gt;  $ make    : Factor w/ 30 levels "amc","audi","bmw",..: 6 4 22 1 12 12 6 22 23 1 ...</span></span>
<span><span class="co">#&gt;  $ model   : chr  "chevelle" "skylark" "satellite" "rebel" ...</span></span>
<span><span class="co">#&gt;  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...</span></span>
<span><span class="co">#&gt;  $ cylinder: int  8 8 8 8 8 8 8 8 8 8 ...</span></span>
<span><span class="co">#&gt;  $ engine  : num  307 350 318 304 302 429 454 440 455 390 ...</span></span>
<span><span class="co">#&gt;  $ horse   : int  130 165 150 150 140 198 220 215 225 190 ...</span></span>
<span><span class="co">#&gt;  $ weight  : int  3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ...</span></span>
<span><span class="co">#&gt;  $ accel   : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...</span></span>
<span><span class="co">#&gt;  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...</span></span>
<span><span class="co">#&gt;  $ origin  : Factor w/ 3 levels "Amer","Eur","Japan": 1 1 1 1 1 1 1 1 1 1 ...</span></span></code></pre></div>
<div class="section level3">
<h3 id="fit-a-model">Fit a model<a class="anchor" aria-label="anchor" href="#fit-a-model"></a>
</h3>
<p>Fit a model predicting gas mileage (<code>mpg</code>) from the number of cylinders, engine displacement, horsepower, weight, time to accelerate from 0 ‚Äì 60 mph and model year (1970‚Äì1982). Perhaps surprisingly, only <code>weight</code> and <code>year</code> appear to significantly predict gas mileage. What‚Äôs going on here?</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cars.mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span> <span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">cylinder</span> <span class="op">+</span> <span class="va">engine</span> <span class="op">+</span> <span class="va">horse</span> <span class="op">+</span> <span class="va">weight</span> <span class="op">+</span> <span class="va">accel</span> <span class="op">+</span> <span class="va">year</span>, </span>
<span>                data<span class="op">=</span><span class="va">cars</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html" class="external-link">Anova</a></span><span class="op">(</span><span class="va">cars.mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: mpg</span></span>
<span><span class="co">#&gt;           Sum Sq  Df F value Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; cylinder      12   1    0.99   0.32    </span></span>
<span><span class="co">#&gt; engine        13   1    1.09   0.30    </span></span>
<span><span class="co">#&gt; horse          0   1    0.00   0.98    </span></span>
<span><span class="co">#&gt; weight      1214   1  102.84 &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; accel          8   1    0.70   0.40    </span></span>
<span><span class="co">#&gt; year        2419   1  204.99 &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; Residuals   4543 385                   </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p><code><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html" class="external-link">lmtest::coeftest()</a></code> shows the coefficients, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><msub><mi>Œ≤</mi><mi>j</mi></msub><mo accent="true">ÃÇ</mo></mover><annotation encoding="application/x-tex">\hat{\beta_j}</annotation></semantics></math>, their standard errors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><msub><mi>Œ≤</mi><mi>j</mi></msub><mo accent="true">ÃÇ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">s(\hat{\beta_j})</annotation></semantics></math> and associated <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> statistics, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mover><msub><mi>Œ≤</mi><mi>j</mi></msub><mo accent="true">ÃÇ</mo></mover><mi>/</mi><mi>s</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><msub><mi>Œ≤</mi><mi>j</mi></msub><mo accent="true">ÃÇ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">t = \hat{\beta_j} / s(\hat{\beta_j})</annotation></semantics></math>. As we will see, the standard errors of the non-significant predictors have been inflated due to high multiple correlations among the predictors, making the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> statistics smaller.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">lmtest</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/coeftest.html" class="external-link">coeftest</a></span><span class="op">(</span><span class="va">cars.mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; t test of coefficients:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept) -1.45e+01   4.76e+00   -3.05   0.0024 ** </span></span>
<span><span class="co">#&gt; cylinder    -3.30e-01   3.32e-01   -0.99   0.3212    </span></span>
<span><span class="co">#&gt; engine       7.68e-03   7.36e-03    1.04   0.2973    </span></span>
<span><span class="co">#&gt; horse       -3.91e-04   1.38e-02   -0.03   0.9775    </span></span>
<span><span class="co">#&gt; weight      -6.79e-03   6.70e-04  -10.14   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; accel        8.53e-02   1.02e-01    0.84   0.4038    </span></span>
<span><span class="co">#&gt; year         7.53e-01   5.26e-02   14.32   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="correlation-matrix">Correlation matrix<a class="anchor" aria-label="anchor" href="#correlation-matrix"></a>
</h3>
<p>It is often recommended to examine the correlation matrix of the predictors to diagnose collinearity problems. In the general case, this advice is misguided, because it is not the 0-order correlations that matter, but rather the <strong>multiple correlations</strong> predicting each independent variable from the others, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="normal">others</mtext></mrow></msub><annotation encoding="application/x-tex">R_{x_j | \text{others}}</annotation></semantics></math>.</p>
<p>Nonetheless, it is instructive to examine the correlations.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">R</span> <span class="op">&lt;-</span> <span class="va">cars</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">cylinder</span><span class="op">:</span><span class="va">year</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">tidyr</span><span class="fu">::</span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/drop_na.html" class="external-link">drop_na</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cor</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fl">100</span> <span class="op">*</span> <span class="va">R</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt;          cylinder engine horse weight accel year</span></span>
<span><span class="co">#&gt; cylinder      100     95    84     90   -52  -36</span></span>
<span><span class="co">#&gt; engine         95    100    90     93   -56  -38</span></span>
<span><span class="co">#&gt; horse          84     90   100     87   -70  -42</span></span>
<span><span class="co">#&gt; weight         90     93    87    100   -43  -32</span></span>
<span><span class="co">#&gt; accel         -52    -56   -70    -43   100   30</span></span>
<span><span class="co">#&gt; year          -36    -38   -42    -32    30  100</span></span></code></pre></div>
<p>Or, better yet, use <code><a href="https://rdrr.io/pkg/corrplot/man/corrplot.mixed.html" class="external-link">corrplot::corrplot.mixed()</a></code> to visualize them, using color and shading of glyphs,</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/corrplot/man/corrplot.mixed.html" class="external-link">corrplot.mixed</a></span><span class="op">(</span><span class="va">R</span>, lower <span class="op">=</span> <span class="st">"square"</span>, upper <span class="op">=</span> <span class="st">"ellipse"</span>, tl.col <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span></span></code></pre></div>
<p><img src="reference/figures/README-cars-corrgram-1.png" width="60%"></p>
<p>The message here seems to be that there are two clusters of predictors with high correlations: {<code>cylinder</code>, <code>engine</code>, <code>horse</code> and <code>weight</code>}, and {<code>accel</code>, <code>year</code>}.</p>
</div>
<div class="section level3">
<h3 id="variance-inflation-factors">Variance inflation factors<a class="anchor" aria-label="anchor" href="#variance-inflation-factors"></a>
</h3>
<p>Variance inflation factors measure the effect of multicollinearity on the standard errors of the estimated coefficients and are proportional to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msubsup><mi>R</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false" form="prefix">|</mo><mtext mathvariant="normal">others</mtext></mrow><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">1 / (1 - R^2_{x_j | \text{others}})</annotation></semantics></math>.</p>
<p>We check the variance inflation factors, using <code><a href="https://rdrr.io/pkg/car/man/vif.html" class="external-link">car::vif()</a></code>. We see that most predictors have very high VIFs, indicating moderately severe multicollinearity.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html" class="external-link">vif</a></span><span class="op">(</span><span class="va">cars.mod</span><span class="op">)</span></span>
<span><span class="co">#&gt; cylinder   engine    horse   weight    accel     year </span></span>
<span><span class="co">#&gt;    10.63    19.64     9.40    10.73     2.63     1.24</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html" class="external-link">vif</a></span><span class="op">(</span><span class="va">cars.mod</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; cylinder   engine    horse   weight    accel     year </span></span>
<span><span class="co">#&gt;     3.26     4.43     3.07     3.28     1.62     1.12</span></span></code></pre></div>
<p>According to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mtext mathvariant="normal">VIF</mtext></msqrt><annotation encoding="application/x-tex">\sqrt{\text{VIF}}</annotation></semantics></math>, the standard error of <code>cylinder</code> has been multiplied by 3.26 and it‚Äôs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-value divided by this number, compared with the case when all predictors are uncorrelated. <code>engine</code>, <code>horse</code> and <code>weight</code> suffer a similar fate.</p>
</div>
<div class="section level3">
<h3 id="collinearity-diagnostics">Collinearity diagnostics<a class="anchor" aria-label="anchor" href="#collinearity-diagnostics"></a>
</h3>
<p>The diagnostic measures introduced by Belsley (1991) are based on the eigenvalues <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Œª</mi><mn>2</mn></msub><mo>,</mo><mi>‚Ä¶</mi><msub><mi>Œª</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_1, \lambda_2, \dots \lambda_p</annotation></semantics></math> of the correlation matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mi>X</mi></msub><annotation encoding="application/x-tex">R_{X}</annotation></semantics></math> of the predictors (preferably centered and scaled, and not including the constant term for the intercept), and the corresponding eigenvectors in the columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ùêï</mi><mrow><mi>p</mi><mo>√ó</mo><mi>p</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{V}_{p \times p}</annotation></semantics></math>.</p>
<p><code><a href="reference/colldiag.html">colldiag()</a></code> calculates:</p>
<ul>
<li>
<p><strong>Condition indices</strong>: The smallest of the eigenvalues, those for which <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mi>j</mi></msub><mo>‚âà</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_j \approx 0</annotation></semantics></math>, indicate collinearity and the number of small values indicates the number of near collinear relations. Because the sum of the eigenvalues, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ£</mi><msub><mi>Œª</mi><mi>i</mi></msub><mo>=</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">\Sigma \lambda_i = p</annotation></semantics></math> increases with the number of predictors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, it is useful to scale them all in relation to the largest. This leads to <em>condition indices</em>, defined as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∫</mi><mi>j</mi></msub><mo>=</mo><msqrt><mrow><msub><mi>Œª</mi><mn>1</mn></msub><mi>/</mi><msub><mi>Œª</mi><mi>j</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">\kappa_j = \sqrt{ \lambda_1 / \lambda_j}</annotation></semantics></math>. These have the property that the resulting numbers have common interpretations regardless of the number of predictors.</p>
<ul>
<li><p>For completely uncorrelated predictors, all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∫</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\kappa_j = 1</annotation></semantics></math>.</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∫</mi><mi>j</mi></msub><mo>‚Üí</mo><mi>‚àû</mi></mrow><annotation encoding="application/x-tex">\kappa_j \rightarrow \infty</annotation></semantics></math> as any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mi>k</mi></msub><mo>‚Üí</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_k \rightarrow 0</annotation></semantics></math>.</p></li>
<li><p>In terms of the eigen-decomposition, variance inflation factors can be expressed as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">VIF</mtext><mi>j</mi></msub><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><mfrac><msubsup><mi>V</mi><mrow><mi>j</mi><mi>k</mi></mrow><mn>2</mn></msubsup><msub><mi>Œª</mi><mi>k</mi></msub></mfrac><mspace width="0.278em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{VIF}_j = \sum_{k=1}^{p} \frac{V^2_{jk}}{\lambda_k} \; .
</annotation></semantics></math></p></li>
</ul>
</li>
<li><p><strong>Variance decomposition proportions</strong>: Large VIFs indicate variables that are involved in <em>some</em> nearly collinear relations, but they don‚Äôt indicate <em>which</em> other variable(s) each is involved with. For this purpose, Belsley et. al.¬†(1980) and Belsley (1991) proposed calculation of the proportions of variance of each variable associated with each principal component as a decomposition of the coefficient variance for each dimension.</p></li>
</ul>
<p>For the current model, the usual display contains both the condition indices and variance proportions. However, even for a small example, it is often difficult to know what numbers to pay attention to.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">cd</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/colldiag.html">colldiag</a></span><span class="op">(</span><span class="va">cars.mod</span>, center<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Condition</span></span>
<span><span class="co">#&gt; Index    Variance Decomposition Proportions</span></span>
<span><span class="co">#&gt;           cylinder engine horse weight accel year </span></span>
<span><span class="co">#&gt; 1   1.000 0.005    0.003  0.005 0.004  0.009 0.010</span></span>
<span><span class="co">#&gt; 2   2.252 0.004    0.002  0.000 0.007  0.022 0.787</span></span>
<span><span class="co">#&gt; 3   2.515 0.004    0.001  0.002 0.010  0.423 0.142</span></span>
<span><span class="co">#&gt; 4   5.660 0.309    0.014  0.306 0.087  0.063 0.005</span></span>
<span><span class="co">#&gt; 5   8.342 0.115    0.000  0.654 0.715  0.469 0.052</span></span>
<span><span class="co">#&gt; 6  10.818 0.563    0.981  0.032 0.176  0.013 0.004</span></span></code></pre></div>
<p>Belsley (1991) recommends that the sources of collinearity be diagnosed (a) only for those components with large <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ∫</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\kappa_j</annotation></semantics></math>, and (b) for those components for which the variance proportion is large (say, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚â•</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\ge 0.5</annotation></semantics></math>) on <em>two</em> or more predictors. The print method for <code>"colldiag"</code> objects has a <code>fuzz</code> argument controlling this.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">cd</span>, fuzz <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="co">#&gt; Condition</span></span>
<span><span class="co">#&gt; Index    Variance Decomposition Proportions</span></span>
<span><span class="co">#&gt;           cylinder engine horse weight accel year </span></span>
<span><span class="co">#&gt; 1   1.000  .        .      .     .      .     .   </span></span>
<span><span class="co">#&gt; 2   2.252  .        .      .     .      .    0.787</span></span>
<span><span class="co">#&gt; 3   2.515  .        .      .     .      .     .   </span></span>
<span><span class="co">#&gt; 4   5.660  .        .      .     .      .     .   </span></span>
<span><span class="co">#&gt; 5   8.342  .        .     0.654 0.715   .     .   </span></span>
<span><span class="co">#&gt; 6  10.818 0.563    0.981   .     .      .     .</span></span></code></pre></div>
<p>The mystery is solved: There are two nearly collinear relations among the predictors, corresponding to the two smallest dimensions.</p>
<ul>
<li>Dimension 5 reflects the high correlation between horsepower and weight,</li>
<li>Dimension 6 reflects the high correlation between number of cylinders and engine displacement.</li>
</ul>
<p>Note that the high variance proportion for <code>year</code> (0.787) on the second component creates no problem and should be ignored because (a) the condition index is low and (b) it shares nothing with other predictors.</p>
</div>
<div class="section level3">
<h3 id="tableplot">Tableplot<a class="anchor" aria-label="anchor" href="#tableplot"></a>
</h3>
<p>The simplified tabular display above can be improved to make the patterns of collinearity more visually apparent and to signify warnings directly to the eyes. A ‚Äútableplot‚Äù (Kwan et-al., 2009) is a semi-graphic display that presents numerical information in a table using shapes proportional to the value in a cell and other visual attributes (shape type, color fill, and so forth) to encode other information.</p>
<p>For collinearity diagnostics, these show:</p>
<ul>
<li>the condition indices, using using <em>squares</em> whose background color is red for condition indices &gt; 10, green for values &gt; 5 and green otherwise, reflecting danger, warning and OK respectively. The value of the condition index is encoded within this using a white square whose side is proportional to the value (up to some maximum value, <code>cond.max</code>).</li>
<li>Variance decomposition proportions are shown by filled <em>circles</em> whose radius is proportional to those values and are filled (by default) with shades ranging from white through pink to red. Rounded values of those diagnostics are printed in the cells.</li>
</ul>
<p>The tableplot below encodes all the information from the values of <code><a href="reference/colldiag.html">colldiag()</a></code> printed above (but using <code>prop.col</code> color breaks such that variance proportions &lt; 0.3 are shaded white). The visual message is that one should attend to collinearities with large condition indices <strong>and</strong> large variance proportions implicating two or more predictors.</p>
<!-- ```{r cars-tableplot0} -->
<!-- knitr::include_graphics("man/figures/cars-tableplot.png") -->
<!-- ``` -->
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/tableplot.html">tableplot</a></span><span class="op">(</span><span class="va">cd</span>, title <span class="op">=</span> <span class="st">"Tableplot of cars data"</span>, cond.max <span class="op">=</span> <span class="fl">30</span> <span class="op">)</span></span></code></pre></div>
<p><img src="reference/figures/README-cars-tableplot-1.png" width="100%"></p>
</div>
<div class="section level3">
<h3 id="collinearity-biplot">Collinearity biplot<a class="anchor" aria-label="anchor" href="#collinearity-biplot"></a>
</h3>
<p>The standard biplot (Gabriel, 1971; Gower&amp; Hand 1996) can be regarded as a multivariate analog of a scatterplot, obtained by projecting a multivariate sample into a low-dimensional space (typically of 2 or 3 dimensions) accounting for the greatest variance in the data. With the symmetric (PCA) scaling used here, this is equivalent to a plot of principal component scores of the mean-centered matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>ùêó</mi><mo accent="true">ÃÉ</mo></mover><mo>=</mo><mi>ùêó</mi><mo>‚àí</mo><mover><mi>ùêó</mi><mo accent="true">‚Äæ</mo></mover></mrow><annotation encoding="application/x-tex">\widetilde{\mathbf{X}} = \mathbf{X} - \bar{\mathbf{X}}</annotation></semantics></math> of predictors for the observations (shown as points or case labels), together with principal component coefficients for the variables (shown as vectors) in the same 2D (or 3D) space.</p>
<p>However the standard biplot is less useful for visualizing the relations among the predictors that lead to nearly collinear relations. Instead, biplots of the <strong>smallest dimensions</strong> show these relations directly, and can show other features of the data as well, such as outliers and leverage points. We use <code>prcomp(X, scale.=TRUE)</code> to obtain the PCA of the correlation matrix of the predictors:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cars.X</span> <span class="op">&lt;-</span> <span class="va">cars</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="fu"><a href="https://tidyselect.r-lib.org/reference/where.html" class="external-link">where</a></span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">mpg</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">tidyr</span><span class="fu">::</span><span class="fu"><a href="https://tidyr.tidyverse.org/reference/drop_na.html" class="external-link">drop_na</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">cars.pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">cars.X</span>, scale. <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">cars.pca</span></span>
<span><span class="co">#&gt; Standard deviations (1, .., p=6):</span></span>
<span><span class="co">#&gt; [1] 2.070 0.911 0.809 0.367 0.245 0.189</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Rotation (n x k) = (6 x 6):</span></span>
<span><span class="co">#&gt;             PC1    PC2    PC3    PC4     PC5     PC6</span></span>
<span><span class="co">#&gt; cylinder -0.454 0.1869 -0.168  0.659 -0.2711  0.4725</span></span>
<span><span class="co">#&gt; engine   -0.467 0.1628 -0.134  0.193 -0.0109 -0.8364</span></span>
<span><span class="co">#&gt; horse    -0.462 0.0177  0.123 -0.620 -0.6123  0.1067</span></span>
<span><span class="co">#&gt; weight   -0.444 0.2598 -0.278 -0.350  0.6860  0.2539</span></span>
<span><span class="co">#&gt; accel     0.330 0.2098 -0.865 -0.143 -0.2774 -0.0337</span></span>
<span><span class="co">#&gt; year      0.237 0.9092  0.335 -0.025 -0.0624 -0.0142</span></span></code></pre></div>
<p>The standard deviations above are the square roots <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><msub><mi>Œª</mi><mi>j</mi></msub></msqrt><annotation encoding="application/x-tex">\sqrt{\lambda_j}</annotation></semantics></math> of the eigenvalues of the correlation matrix, and are returned in the <code>sdev</code> component of the <code>"prcomp"</code> object. The eigenvectors are returned in the <code>rotation</code> component, whose directions are arbitrary.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Make labels for dimensions include % of variance</span></span>
<span><span class="va">pct</span> <span class="op">&lt;-</span> <span class="fl">100</span> <span class="op">*</span><span class="op">(</span><span class="va">cars.pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">cars.pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">lab</span> <span class="op">&lt;-</span> <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"Dimension {1:6} ({round(pct, 2)}%)"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Direction of eigenvectors is arbitrary. Reflect them</span></span>
<span><span class="va">cars.pca</span><span class="op">$</span><span class="va">rotation</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="va">cars.pca</span><span class="op">$</span><span class="va">rotation</span></span></code></pre></div>
<p>The collinearity biplot is then constructed as follows:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">op</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>lwd <span class="op">=</span> <span class="fl">2</span>, xpd <span class="op">=</span> <span class="cn">NA</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/biplot.html" class="external-link">biplot</a></span><span class="op">(</span><span class="va">cars.pca</span>,</span>
<span>       choices<span class="op">=</span><span class="fl">6</span><span class="op">:</span><span class="fl">5</span>,           <span class="co"># only the last two dimensions</span></span>
<span>       scale<span class="op">=</span><span class="fl">0.5</span>,             <span class="co"># symmetric biplot scaling</span></span>
<span>       cex<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.6</span>, <span class="fl">1</span><span class="op">)</span>,         <span class="co"># character sizes for points and vectors</span></span>
<span>       col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"blue"</span><span class="op">)</span>,</span>
<span>       expand <span class="op">=</span> <span class="fl">1.7</span>,          <span class="co"># expand variable vectors for visibility</span></span>
<span>       xlab <span class="op">=</span> <span class="va">lab</span><span class="op">[</span><span class="fl">6</span><span class="op">]</span>,</span>
<span>       ylab <span class="op">=</span> <span class="va">lab</span><span class="op">[</span><span class="fl">5</span><span class="op">]</span>,</span>
<span>       xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.7</span>, <span class="fl">0.5</span><span class="op">)</span>,</span>
<span>       ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.8</span>, <span class="fl">0.5</span><span class="op">)</span></span>
<span>      <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span><span class="va">op</span><span class="op">)</span></span></code></pre></div>
<p><img src="reference/figures/README-cars-biplot-1.png" width="100%"></p>
<p>The projections of the variable vectors on the Dimension 5 and Dimension 6 axes are proportional to their variance proportions shown above. The relative lengths of these variable vectors can be considered to indicate the extent to which each variable contributes to collinearity for these two near-singular dimensions.</p>
<p>Thus, we see again that Dimension 6 is largely determined by <code>engine</code> size, with a substantial (negative) relation to <code>cylinder</code>. Dimension 5 has its‚Äô strongest relations to <code>weight</code> and <code>horse</code>.</p>
<p>Moreover, there is one observation, #20, that stands out as an outlier in predictor space, far from the centroid. It turns out that this vehicle, a Buick Estate wagon, is an early-year (1970) American behemoth, with an 8-cylinder, 455 cu. in, 225 horse-power engine, and able to go from 0 to 60 mph in 10 sec.¬†(Its MPG is only slightly under-predicted from the regression model, however.)</p>
</div>
<div class="section level3">
<h3 id="remedies-for-collinearity-what-to-do">Remedies for collinearity: What to do?<a class="anchor" aria-label="anchor" href="#remedies-for-collinearity-what-to-do"></a>
</h3>
<p>Collinearity is often a <strong>data</strong> problem, for which there is no magic cure. Nevertheless there are some general guidelines and useful techniques to address this problem.</p>
<ul>
<li><p><strong>Pure prediction</strong>: If we are only interested in predicting / explaining an outcome, and not the model coefficients or which are ‚Äúsignificant‚Äù, collinearity can be largely ignored. The fitted values are unaffected by collinearity.</p></li>
<li>
<p><strong>structural collinearity</strong>: Sometimes collinearity results from structural relations among the variables:</p>
<ul>
<li><p>For example, polynomial terms, like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>,</mo><msup><mi>x</mi><mn>2</mn></msup><mo>,</mo><msup><mi>x</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">x, x^2, x^3</annotation></semantics></math> or interaction terms like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>*</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, x_1 * x_2</annotation></semantics></math> are necessarily correlated. A simple cure is to <em>center</em> the predictors at their means, using <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>‚àí</mo><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mo>,</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>‚àí</mo><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>‚àí</mo><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">x - \bar{x}, (x - \bar{x})^2, (x - \bar{x})^3</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo>‚àí</mo><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>*</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo>‚àí</mo><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(x_1 - \bar{x}_1), (x_2 - \bar{x}_2), (x_1 - \bar{x}_1) * (x_2 - \bar{x}_2)</annotation></semantics></math></p></li>
<li><p>When some predictors share a common cause, as in GNP or population in time-series or cross-national data, you can reduce collinearity by re-defining predictors to reflect <em>per capita measures</em>.</p></li>
</ul>
</li>
<li>
<p><strong>Model re-specification</strong>:</p>
<ul>
<li><p>Drop one or more regressors that have a high VIF if they are not deemed to be essential</p></li>
<li><p>Replace highly correlated regressors with linear combination(s) of them. For example, two related variables, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>2</mn></msub><annotation encoding="application/x-tex">x_2</annotation></semantics></math> can be replaced without any loss of information by replacing them with their sum and difference, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">z_1 = x_1 + x_2</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>2</mn></msub><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">z_2 = x_1 - x_2</annotation></semantics></math>.</p></li>
</ul>
</li>
<li>
<p><strong>Statistical remedies</strong>:</p>
<ul>
<li><p>Transform the predictors to uncorrelated principal components</p></li>
<li><p>use <strong>regularization methods</strong> such as ridge regression and lasso, which correct for collinearity by introducing shrinking coefficients towards 0, introducing a small amount of bias, . See the <a href="https://CRAN.R-project.org/package=genridge" class="external-link">genridge</a> package and its <a href="https://friendly.github.io/genridge/" class="external-link"><code>pkgdown</code> documentation</a> for visualization methods.</p></li>
<li><p>use Bayesian regression; if multicollinearity prevents a regression coefficient from being estimated precisely, then a prior on that coefficient will help to reduce its posterior variance.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Belsley, D.A., Kuh, E. and Welsch, R. (1980). <em>Regression Diagnostics</em>, New York: John Wiley &amp; Sons.</p>
<p>Belsley, D.A. (1991). <em>Conditioning diagnostics, collinearity and weak data in regression</em>. New York: John Wiley &amp; Sons.</p>
<p>Friendly, M., &amp; Kwan, E. (2009). ‚ÄúWhere‚Äôs Waldo: Visualizing Collinearity Diagnostics.‚Äù <em>The American Statistician</em>, <strong>63</strong>, 56‚Äì65. Online: <a href="https://www.datavis.ca/papers/viscollin-tast.pdf" class="external-link uri">https://www.datavis.ca/papers/viscollin-tast.pdf</a>. Supp. materials: <a href="https://www.datavis.ca/papers/viscollin/" class="external-link uri">https://www.datavis.ca/papers/viscollin/</a></p>
<p>Gabriel, K. R. (1971). The Biplot Graphic Display of Matrices with Application to Principal Components Analysis. <em>Biometrics</em>, <strong>58</strong>, 453‚Äì467.</p>
<p>Gower, J. C., &amp; Hand, D. J. (1996). <em>Biplots</em>. London: Chapman &amp; Hall.</p>
<p>Kwan, E., Lu, I. R. R., &amp; Friendly, M. (2009). Tableplot: A new tool for assessing precise predictions. <em>Zeitschrift F√ºr Psychologie / Journal of Psychology</em>, <strong>217</strong>, 38‚Äì48.</p>
</div>
</div>

  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=VisCollin" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/friendly/VisCollin/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/friendly/VisCollin/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li>GPL (&gt;=3)</li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing VisCollin</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Michael Friendly <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0002-3237-0941" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a>  </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://lifecycle.r-lib.org/articles/stages.html#stable" class="external-link"><img src="https://img.shields.io/badge/lifecycle-stable-green.svg" alt="Lifecycle: stable"></a></li>
<li><a href="https://www.gnu.org/licenses/gpl-2.0.html" class="external-link"><img src="https://img.shields.io/badge/license-GPL%20%28%3E=%202%29-brightgreen.svg?style=flat" alt="License"></a></li>
<li><a href="https://cran.r-project.org/package=VisCollin" class="external-link"><img src="https://www.r-pkg.org/badges/version/VisCollin" alt="CRAN"></a></li>
<li><a href="https://github.com/friendly/VisCollin" class="external-link"><img src="https://img.shields.io/github/last-commit/friendly/VisCollin" alt="Last Commit"></a></li>
<li><a href="https://www.r-pkg.org:443/pkg/VisCollin" class="external-link"><img src="https://cranlogs.r-pkg.org/badges/statquotes?color=brightgreen" alt="Downloads"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Michael Friendly.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
